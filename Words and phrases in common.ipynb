{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e27dc539-1bd9-4e8d-89e7-1bbce30cbf45",
   "metadata": {},
   "source": [
    "# Run the code below once as a setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c35032f4-bae4-4cab-886f-bbc16cd93357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk) (4.62.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7968c311-e7fc-445e-ab84-7577d9c1299b",
   "metadata": {},
   "source": [
    "# Modifiable version of the working code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c2097e5-d49c-46b9-b404-62aed40bd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many words should the sentences in common contain? 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Words: {'lone', 'night', 'quiet', 'firefly', 'symphony', 'meadow', 'dance', \"firefly's\", \"nature's\", 'played', 'flickered', 'continued'}\n",
      "Common Phrases:\n",
      "1. In a quiet meadow a lone firefly\n",
      "2. Nature's symphony played on as the firefly's\n",
      "3. In a quiet meadow a lone firefly flickered\n",
      "4. a quiet meadow a lone firefly flickered\n",
      "5. played on as the firefly's dance continued\n",
      "6. Nature's symphony played on as the firefly's dance\n",
      "7. Nature's symphony played on as the firefly's dance continued\n",
      "8. symphony played on as the firefly's dance\n",
      "9. symphony played on as the firefly's dance continued\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Function to find common phrases in two texts\n",
    "inp = int(input(\"How many words should the sentences in common contain?\"))\n",
    "\n",
    "def find_common_words(sentence1, sentence2):\n",
    "    # Tokenize the sentences and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokenizer = RegexpTokenizer(r'\\w+\\'?\\w*')  # Modified regular expression\n",
    "    words1 = [word.lower() for word in tokenizer.tokenize(sentence1) if word.lower() not in stop_words]\n",
    "    words2 = [word.lower() for word in tokenizer.tokenize(sentence2) if word.lower() not in stop_words]\n",
    "\n",
    "    # Find common words\n",
    "    common_words = set(words1) & set(words2)\n",
    "\n",
    "    return common_words\n",
    "\n",
    "def find_common_phrases(text1, text2, min_phrase_length=inp):\n",
    "    # Tokenize the texts into sentences\n",
    "    tokenizer = RegexpTokenizer(r'\\w+\\'?\\w*')  # Modified regular expression\n",
    "    tokens1 = tokenizer.tokenize(text1)\n",
    "    tokens2 = tokenizer.tokenize(text2)\n",
    "\n",
    "    common_phrases = set()\n",
    "\n",
    "    for phrase_length in range(min(len(tokens1), len(tokens2)), min_phrase_length - 1, -1):\n",
    "        ngrams1 = [' '.join(tokens1[i:i+phrase_length]) for i in range(len(tokens1) - phrase_length + 1)]\n",
    "        ngrams2 = [' '.join(tokens2[i:i+phrase_length]) for i in range(len(tokens2) - phrase_length + 1)]\n",
    "\n",
    "        common_phrases.update(set(ngrams1) & set(ngrams2))\n",
    "\n",
    "    # Return the common phrases that meet the minimum length criteria\n",
    "    common_phrases = [phrase for phrase in common_phrases if len(phrase.split()) >= min_phrase_length]\n",
    "\n",
    "    return common_phrases\n",
    "\n",
    "## Example texts\n",
    "sentence1 = \"In a quiet meadow, a lone firefly flickered. It danced, illuminating the night with its soft glow. A tiny owl watched, enchanted by the shimmering light. Nature's symphony played on as the firefly's dance continued, a secret performance in the moonlight.\"\n",
    "sentence2 = \"In a quiet meadow, a lone firefly flickered. The child smiled, feeling the magic of the night. It was a happy gathering. Nature's symphony played on as the firefly's dance continued.\"\n",
    "\n",
    "## Have the user input their desired texts\n",
    "\n",
    "# sentence1 = input(\"Enter the first text: \")\n",
    "# sentence2 = input(\"Enter the second text: \")\n",
    "\n",
    "\n",
    "common_words = find_common_words(sentence1, sentence2)\n",
    "common_phrases = find_common_phrases(text1, text2, min_phrase_length=inp)\n",
    "\n",
    "print(\"Common Words:\", common_words)\n",
    "print(\"Common Phrases:\")\n",
    "for i, phrase in enumerate(common_phrases, 1):\n",
    "    print(f\"{i}. {phrase}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
